# –û—Ç—á–µ—Ç –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É –∑–∞–¥–∞–Ω–∏—é ETL (4 –º–æ–¥—É–ª—å)

### –ó–∞–¥–∞–Ω–∏–µ 1. –†–∞–±–æ—Ç–∞ —Å Yandex DataTransfer

- –í—Ä—É—á–Ω—É—é —Å–æ–∑–¥–∞–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ –≤ YDB
   <details>
    <summary><i>SQL-—Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã</i></summary>

   ### sql-—Å–∫—Ä–∏–ø—Ç —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã –≤ YDB
    ```sql
    CREATE TABLE weather_data (
    location Utf8,
    date_time Timestamp,
    temperature_c Double,
    humidity_pct Double,
    precipitation_mm Double,
    wind_speed_kmh Double,
    PRIMARY KEY (location, date_time)
    );
    ```
  </details> 
  
- –í —Å–æ–∑–¥–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Å –ø–æ–º–æ—â—å—é CLI –∑–∞–≥—Ä—É–∂–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç transaction_v2
  <details>
    <summary><i>bash —Å–∫—Ä–∏–ø—Ç</i></summary>

    ### bash-—Å–∫—Ä–∏–ø—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
    ```bash
    ydb --endpoint grpcs://ydb.serverless.yandexcloud.net:2135 \
    --database /ru-central1/b1gfhcse172450fifcje/etn3p0qk8go18m3m7lfs \
    --sa-key-file ~/authorized_key.json \
    import file csv \
    --path weather_data \
    --delimiter "," \
    --skip-rows 1 \
    --null-value "" \
    "$(pwd)/weather_data.csv"
     ```
  </details> 

- –°–æ–∑–¥–∞–Ω —Ç—Ä–∞–Ω—Å—Ñ–µ—Ä –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –≤ YDB –∏ –ø—Ä–∏–µ–º–Ω–∏–∫–æ–º –≤ Object Storage
      <details>
    	<summary><i>–¢—É—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç—ã —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ –∑–∞–¥–∞—á–∏</i></summary>
	  - ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_1/datatransfer.PNG)
  
    - ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_1/datatransfer1.PNG)
  
  	- ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_1/datatransfer2.PNG)
 
  	- ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_1/file.PNG)
 
  	- ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_1/bash_—Å–∫—Ä–∏–ø—Ç.PNG)
  	 </details> 


### –ó–∞–¥–∞–Ω–∏–µ 2.  üñ•Ô∏è –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —Å Yandex Data Processing –ø—Ä–∏ –ø–æ–º–æ—â–∏ Apache AirFlow

- –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ (Managed service for Airflow)
      <details>
    	<summary><i>–¢—É—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç</i></summary>
	    	- ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_2/airflow.PNG)
  	 </details> 
    
- –°–æ–∑–¥–∞–Ω DAG **DATA_INGEST**, –∫–æ—Ç–æ—Ä—ã–π:
    - –°–æ–∑–¥–∞–µ—Ç Data Proc –∫–ª–∞—Å—Ç–µ—Ä.      
    - –ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–∞ –∫–ª–∞—Å—Ç–µ—Ä–µ PySpark-–∑–∞–¥–∞–Ω–∏–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ Parquet-—Ñ–∞–π–ª–∞.
    - –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã –∑–∞–¥–∞–Ω–∏—è —É–¥–∞–ª—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä.
  <details>
    	<summary><i>–¢—É—Ç —Ç–µ–∫—Å—Ç DAG</i></summary>
  
	 ### Data-proc-DAG.py
  
	 ```python
	  import uuid
    import datetime
    from airflow import DAG
    from airflow.utils.trigger_rule import TriggerRule
    from airflow.providers.yandex.operators.yandexcloud_dataproc import (
    DataprocCreateClusterOperator,
    DataprocCreatePysparkJobOperator,
    DataprocDeleteClusterOperator,
    )

    # –î–∞–Ω–Ω—ã–µ –≤–∞—à–µ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    YC_DP_AZ = 'ru-central1-a'
    YC_DP_SSH_PUBLIC_KEY = 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAICkmBqc3tcoxT2HR5ZJmIoc8s6JQA2QpXo0LieuQ0uQX'
    YC_DP_SUBNET_ID = 'e9ba04povek3lmtgcii4'
    YC_DP_SA_ID = 'ajedll5qllfivmvnm982'
    YC_DP_METASTORE_URI = '10.128.0.14'
    YC_BUCKET = 'databacket'

    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ DAG
    with DAG(
          'DATA_INGEST',
          schedule_interval='@hourly',
          tags=['data-processing-and-airflow'],
          start_date=datetime.datetime.now(),
          max_active_runs=1,
          catchup=False
    ) as ingest_dag:
    # 1 —ç—Ç–∞–ø: —Å–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ Yandex Data Proc
    create_spark_cluster = DataprocCreateClusterOperator(
         task_id='dp-cluster-create-task',
         cluster_name=f'tmp-dp-{uuid.uuid4()}',
         cluster_description='–í—Ä–µ–º–µ–Ω–Ω—ã–π –∫–ª–∞—Å—Ç–µ—Ä –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è PySpark-–∑–∞–¥–∞–Ω–∏—è –ø–æ–¥ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–µ–π Managed Service for Apache Airflow‚Ñ¢',
         ssh_public_keys=YC_DP_SSH_PUBLIC_KEY,
         service_account_id=YC_DP_SA_ID,
         subnet_id=YC_DP_SUBNET_ID,
         s3_bucket=YC_BUCKET,
         zone=YC_DP_AZ,
         cluster_image_version='2.1',
         masternode_resource_preset='s2.small',  # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–µ—Å—É—Ä—Å–Ω—ã–π –ø—Ä–µ—Å–µ—Ç
         masternode_disk_type='network-hdd',
         masternode_disk_size=32,  # —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∏—Å–∫–∞
         computenode_resource_preset='s2.small',  # —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–µ—Å—É—Ä—Å–Ω—ã–π –ø—Ä–µ—Å–µ—Ç
         computenode_disk_type='network-hdd',
         computenode_disk_size=32,  # —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä –¥–∏—Å–∫–∞
         computenode_count=1,  # —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤
         computenode_max_hosts_count=3,  # —É–º–µ–Ω—å—à–µ–Ω–Ω–æ–µ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ
         services=['YARN', 'SPARK'],
         datanode_count=0,
         properties={
             'spark:spark.hive.metastore.uris': f'thrift://{YC_DP_METASTORE_URI}:9083',
         },
     )

    # 2 —ç—Ç–∞–ø: –∑–∞–ø—É—Å–∫ –∑–∞–¥–∞–Ω–∏—è PySpark
    poke_spark_processing = DataprocCreatePysparkJobOperator(
         task_id='dp-cluster-pyspark-task',
         main_python_file_uri=f's3a://{YC_BUCKET}/scripts/clean-data.py',
     )

    # 3 —ç—Ç–∞–ø: —É–¥–∞–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ Yandex Data Processing
    delete_spark_cluster = DataprocDeleteClusterOperator(
         task_id='dp-cluster-delete-task',
         trigger_rule=TriggerRule.ALL_DONE,
     )

    # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ DAG –∏–∑ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –≤—ã—à–µ —ç—Ç–∞–ø–æ–≤
    create_spark_cluster >> poke_spark_processing >> delete_spark_cluster
  	```
</details>

- –í–Ω—É—Ç—Ä–∏ –µ—Å—Ç—å —Å–∫—Ä–∏–ø—Ç-–∑–∞–¥–∞–Ω–∏–µ
  <details>
    <summary><i>–¢—É—Ç —Ç–µ–∫—Å—Ç —Å–∫—Ä–∏–ø—Ç–∞</i></summary>
  
	### clean-data.py
		  
	```python
 	from pyspark.sql import SparkSession
 	from pyspark.sql.functions import col, to_date
 	from pyspark.sql.types import IntegerType, StringType, BooleanType
 	from pyspark.sql.utils import AnalysisException
 	
 	
 	# === Spark session ===
 	spark = SparkSession.builder.appName("Parquet ETL with Logging to S3").getOrCreate()
 	
 	
 	# === –ü—É—Ç–∏ ===
 	source_path = "s3a://databacket/weather_data.csv"
 	target_path = "s3a://databacket/weather_data.parquet"
 	
 	try:
 	    print(f"–ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑: {source_path}")
 	    df = spark.read.option("header", "true").option("inferSchema", "true").csv(source_path)
 	
 	    print("–°—Ö–µ–º–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
 	    df.printSchema()
 	
 	    # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ + —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã YYYYMMDD
 	    df = df.withColumn("location", col("location").cast(StringType())) \
 	           .withColumn("date_time", to_date(col("date_time").cast("string"), "yyyyMMdd")) \
 	           .withColumn("temperature_c", col("temperature_c").DoubleType())) \
 	           .withColumn("humidity_pct", col("humidity_pct").DoubleType())) \
 	           .withColumn("precipitation_mm", col("precipitation_mm").DoubleType())) \
 	           .withColumn("wind_speed_kmh", col("wind_speed_kmh").DoubleType())
 	
 	    print("–°—Ö–µ–º–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:")
 	    df.printSchema()
 	
 	    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
 	    df = df.na.drop()
 	
 	    print("–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è:")
 	    df.show(5)
 	
 	    print(f"–ó–∞–ø–∏—Å—å –≤ Parquet: {target_path}")
 	    df.write.mode("overwrite").parquet(target_path)
 	
 	    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ Parquet.")

 	except AnalysisException as ae:
 	    print("‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞:", ae)
 	except Exception as e:
 	    print("‚ùå –û–±—â–∞—è –æ—à–∏–±–∫–∞:", e)
 
 	spark.stop()
 	```
	
</details> 

  <details>
  	<summary><i>–¢—É—Ç –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã</i></summary>
	
  - ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_2/dag.PNG)
	
  - ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_2/metastore.PNG)

  - ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_2/data_ingest.PNG)
  
  </details> 

  ### –ó–∞–¥–∞–Ω–∏–µ 4. üìä –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Yandex DataLens

  –°–∫—Ä–∏–Ω –ø—Ä–∏–ª–æ–∂–µ–Ω –∏–∑ –¥–æ–º–∞—à–Ω–µ–≥–æ –∑–∞–¥–∞–Ω–∏—è –ø–æ –ø—Ä–µ–¥–º–µ—Ç—É "–°–µ–º–∏–Ω–∞—Ä –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞" —Ç.–∫. —Ç–∞–º –ø—Ä–æ—Å—Ç–æ –ø–æ—Å–∏–º–ø–æ—Ç–∏—á–Ω–µ–µ –ø–æ–ª—É—á–∏–ª–∞—Å—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
  PS –µ—Å–ª–∏ —É—Å–ø–µ—é –ø–µ—Ä–µ–∑–∞–ª—å—é –¥–∞–Ω–Ω—ã–µ –∏ —Å–¥–µ–ª–∞—é –Ω–æ–≤—ã–π –¥–∞—à–±–æ—Ä–¥

  <details>
  	<summary><i>–¢—É—Ç —Å–∫—Ä–∏–Ω—à–æ—Ç</i></summary>
	
  - ![–°–∫—Ä–∏–Ω—à–æ—Ç](Task_4/–î–∞—à–±–æ—Ä–¥.PNG)
	 
  </details> 

  <details>
  	<summary><i>–¢—É—Ç —Å—Å—ã–ª–∫–∞ –Ω–∞ —Ä–µ–ø—É —Å –î–ó –ø–æ –ø—Ä–µ–¥–º–µ—Ç—É "—Å–µ–º–∏–Ω–∞—Ä –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞"</i></summary>
	
    https://github.com/Narasslabone/hse_hw_ms
	 
  </details>
